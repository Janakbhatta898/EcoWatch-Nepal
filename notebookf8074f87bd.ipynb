{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport librosa as lb\nimport librosa.display\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import layers,models,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport tensorflow as tf\nimport os\nimport gc\nfrom PIL import Image as PIL_Image\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:04:09.911897Z","iopub.execute_input":"2026-01-06T15:04:09.912498Z","iopub.status.idle":"2026-01-06T15:04:23.902551Z","shell.execute_reply.started":"2026-01-06T15:04:09.912467Z","shell.execute_reply":"2026-01-06T15:04:23.901902Z"}},"outputs":[{"name":"stderr","text":"2026-01-06 15:04:12.396643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767711852.579966      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767711852.629241      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767711853.055789      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767711853.055832      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767711853.055835      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767711853.055837      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def format_shape(data, target_height=128, target_width=1000):\n    data = np.asarray(data, dtype=np.float32)\n\n\n#1D -> 2D\n    if data.ndim == 1:\n        data = data[np.newaxis, :]\n\n    # Min-max normalization\n    data=(255 * (data - np.min(data)) / (np.max(data) - np.min(data))).astype(np.uint8)\n\n    # Resize rows to target_height\n    rows, cols = data.shape\n    if rows < target_height:\n        reps = int(np.ceil(target_height / rows))\n        data = np.tile(data, (reps, 1))[:target_height, :]\n    elif rows > target_height:\n        data = data[:target_height, :]\n\n    # Resize columns to target_width\n    rows, cols = data.shape\n    if cols < target_width:\n        pad_width = target_width - cols\n        data = np.pad(data, ((0,0), (0, pad_width)), mode=\"constant\")\n    elif cols > target_width:\n        data = data[:, :target_width]\n\n    return data.astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:04:29.401498Z","iopub.execute_input":"2026-01-06T15:04:29.402526Z","iopub.status.idle":"2026-01-06T15:04:29.408295Z","shell.execute_reply.started":"2026-01-06T15:04:29.402485Z","shell.execute_reply":"2026-01-06T15:04:29.407544Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#this function will be responsible to convert all the audios into images\ndef audio_to_image(file=None,max_size=1000,y=None,sr=22050):\n    #loading up the image\n    if not file is None:\n        if y is None:\n            y,sr=librosa.load(file,sr=sr)\n    y = np.asarray(y, dtype=np.float32)\n\n    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=224, n_fft=2048, hop_length=384)\n\n    #next channel which mfcc is which is audio graph that is audible to human\n    mels_db = librosa.power_to_db(mels, ref=np.max)\n\n    mels_pcen = librosa.pcen(mels * (2*31), sr=sr) #frequency ma compress\n\n    mels_delta = librosa.feature.delta(mels_db) #first derivative\n\n     # 3. Enhanced Normalization to 0-255\n    def norm(X):\n        X -= X.min()\n        X /= (X.max() + 1e-9)\n        return (X 255).astype(np.uint8)   # <--------------\n\n    # # Stack: DB, PCEN, and Delta\n    # return np.dstack([norm(mels_db), norm(mels_pcen), norm(mels_delta)])\n\n\n    def normalize(X):\n        x_min, x_max = X.min(), X.max()\n        if x_max - x_min > 0:\n            return (255 * (X - x_min) / (x_max - x_min)).astype(np.uint8)\n        return np.zeros_like(X, dtype=np.uint8)\n\n    layer0 = format_shape(mels_db)\n    layer1 = format_shape(mels_delta)\n    layer2 = format_shape(mels_pcen)\n\n    #this makes cube by taking 3 images and stacking on top of each other\n    final_image = np.dstack([layer0, layer1, layer2]).astype(np.float32)\n    return final_image,y,sr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:04:33.064291Z","iopub.execute_input":"2026-01-06T15:04:33.064902Z","iopub.status.idle":"2026-01-06T15:04:33.072502Z","shell.execute_reply.started":"2026-01-06T15:04:33.064875Z","shell.execute_reply":"2026-01-06T15:04:33.071493Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_55/2750401078.py\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    return (X 255).astype(np.uint8)   # <--------------\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"],"ename":"SyntaxError","evalue":"invalid syntax. Perhaps you forgot a comma? (2750401078.py, line 22)","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"def convert_dir_to_audio(file_path, output_dir):\n    class_names = [d for d in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, d))]\n\n    for i, class_name in enumerate(class_names):\n        class_input_path = os.path.join(file_path, class_name)\n        save_folder = os.path.join(output_dir, class_name)\n        os.makedirs(save_folder, exist_ok=True)\n\n        print(f\"--- Processing Class: {class_name} ---\")\n\n        for file_name in os.listdir(class_input_path):\n            if file_name.endswith((\".wav\", \".m4a\")):\n                full_file_path = os.path.join(class_input_path, file_name)\n\n                try:\n                    # 1. Process Original\n                    # Note: We use 'pixel_data' everywhere now\n                    pixel_data, y, sr = audio_to_image(full_file_path, y=None)\n\n# Save Original\n                    im = PIL_Image.fromarray(pixel_data.astype(np.uint8))\n                    im.save(os.path.join(save_folder, f\"{file_name}_orig.png\"))\n\n# Process Augmentations\n                    for j in range(3): \n                        y_aug = augment_audio(y, sr)\n                        aug_pixeldata, _, _ = audio_to_image(file=None, y=y_aug, sr=sr)\n\n                        im_aug = PIL_Image.fromarray(aug_pixel_data.astype(np.uint8))\n                        # Use f-string to safely combine string and number j\n                        im_aug.save(os.path.join(save_folder, f\"{file_name}aug{j}.png\"))\n\n                        del y_aug, aug_pixel_data \n\n                    # Cleanup the original data for this file\n                    del pixel_data, y\n\n                except Exception as e:\n                    print(f\"Skipping {file_name}: {e}\")\n\n        gc.collect()\n\n    return class_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:04:35.305755Z","iopub.execute_input":"2026-01-06T15:04:35.306526Z","iopub.status.idle":"2026-01-06T15:04:35.313466Z","shell.execute_reply.started":"2026-01-06T15:04:35.306487Z","shell.execute_reply":"2026-01-06T15:04:35.312653Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def augment_audio(y,sr):\n    if np.random.random()>0.5:\n        y=librosa.effects.pitch_shift(y,sr=sr,n_steps=np.random.uniform(-2,2))\n        y = np.asarray(y, dtype=np.float32)\n    y = y * np.random.uniform(0.6, 1.4) # Randomly quieter or louder\n    noise_amp=0.005 * np.random.uniform()*np.amax(y)\n    noise_amp = 0.005 * np.random.uniform() * np.amax(np.abs(y)) if y.size > 0 else 0.0\n    y = y + noise_amp * np.random.normal(size=y.shape).astype(np.float32)\n    return y.astype(np.float32)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:06:21.775954Z","iopub.execute_input":"2026-01-06T15:06:21.776304Z","iopub.status.idle":"2026-01-06T15:06:21.781697Z","shell.execute_reply.started":"2026-01-06T15:06:21.776276Z","shell.execute_reply":"2026-01-06T15:06:21.780941Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def split_data(\n    x,\n    y,\n    test_size=0.2,\n    val_size=0.5,\n    random_state=42,\n    stratify=True\n):\n    x = np.array(x)\n    y = np.array(y)\n\n    strat = y if stratify else None\n\n    # Train / temp split\n    x_train, x_temp, y_train, y_temp = train_test_split(\n        x,\n        y,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=strat\n    )\n\n    # Val / test split\n    x_val, x_test, y_val, y_test = train_test_split(\n        x_temp,\n        y_temp,\n        test_size=val_size,\n        random_state=random_state,\n        stratify=(y_temp if stratify else None)\n    )\n\n    return x_train, y_train, x_val, y_val, x_test, y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:06:26.542587Z","iopub.execute_input":"2026-01-06T15:06:26.542867Z","iopub.status.idle":"2026-01-06T15:06:26.548027Z","shell.execute_reply.started":"2026-01-06T15:06:26.542842Z","shell.execute_reply":"2026-01-06T15:06:26.547284Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def for_single_audio(file):\n    img,y,sr=audio_to_image(file)\n    img_array=np.expand_dims(img_array, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:06:30.528119Z","iopub.execute_input":"2026-01-06T15:06:30.528455Z","iopub.status.idle":"2026-01-06T15:06:30.532532Z","shell.execute_reply.started":"2026-01-06T15:06:30.528428Z","shell.execute_reply":"2026-01-06T15:06:30.531842Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"input=\"/kaggle/input/forest-audio-dataset-added/forestdataset\"\noutput=\"/kaggle/working/processed\"\nclass_names=convert_dir_to_audio(input,output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T14:49:46.940291Z","iopub.execute_input":"2026-01-06T14:49:46.941034Z","iopub.status.idle":"2026-01-06T14:49:47.038386Z","shell.execute_reply.started":"2026-01-06T14:49:46.941000Z","shell.execute_reply":"2026-01-06T14:49:47.037147Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3808979547.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/kaggle/input/forest-audio-dataset-added/forestdataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/kaggle/working/processed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dir_to_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'convert_dir_to_audio' is not defined"],"ename":"NameError","evalue":"name 'convert_dir_to_audio' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"import shutil\n\n# Zip the folder for easy download\nshutil.make_archive(\"/kaggle/working/forest_processed\", 'zip', \"/kaggle/working/processed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:07:15.101826Z","iopub.execute_input":"2026-01-06T15:07:15.102546Z","iopub.status.idle":"2026-01-06T15:07:15.108066Z","shell.execute_reply.started":"2026-01-06T15:07:15.102516Z","shell.execute_reply":"2026-01-06T15:07:15.107386Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"BATCH_SIZE=32\nIMG_SIZE=(224,224)\nSEED=42\nEXTRACT_PATH=\"/kaggle/input/forest-audio-sonograph-final\"\nselected_class=[\"natural sound\",\"unnatural\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:07:20.008516Z","iopub.execute_input":"2026-01-06T15:07:20.009197Z","iopub.status.idle":"2026-01-06T15:07:20.012643Z","shell.execute_reply.started":"2026-01-06T15:07:20.009169Z","shell.execute_reply":"2026-01-06T15:07:20.011938Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#loading up images and dividing them for the neural net\ntrain_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=SEED,\n    image_size=(128, 1000),\n    batch_size=BATCH_SIZE,\n    label_mode=\"binary\",\n    color_mode = 'rgb'\n)\n\n#validation dataset\nvalidation_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=SEED,\n    image_size=(128, 1000),\n    batch_size=BATCH_SIZE,\n    label_mode=\"binary\",\n    color_mode = 'rgb'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:07:22.626412Z","iopub.execute_input":"2026-01-06T15:07:22.626931Z","iopub.status.idle":"2026-01-06T15:07:44.398403Z","shell.execute_reply.started":"2026-01-06T15:07:22.626902Z","shell.execute_reply":"2026-01-06T15:07:44.397622Z"}},"outputs":[{"name":"stdout","text":"Found 12676 files belonging to 2 classes.\nUsing 10141 files for training.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1767712058.733890      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1767712058.737777      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Found 12676 files belonging to 2 classes.\nUsing 2535 files for validation.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#now dividing the validation data set into \ntotal_number_of_batches_in_validation_data=validation_data.cardinality().numpy() #converts total number of batches in the set and converts the tensor into python integer with .numpy function here\nno_of_batches_in_validation_data=total_number_of_batches_in_validation_data//2 #this is a floor division operator\n\n\n#now from the set of batches of the image creating subset into the validation and the test subset\nvalidation=validation_data.take(no_of_batches_in_validation_data)\ntest=validation_data.skip(no_of_batches_in_validation_data)\nprint(f\"Class names: {train_data.class_names}\")\nprint(\"For values:\\n\")\nfor i, class_name in enumerate(train_data.class_names):\n    print(f\"{class_name}:{i}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:07:44.399724Z","iopub.execute_input":"2026-01-06T15:07:44.400000Z","iopub.status.idle":"2026-01-06T15:07:44.411458Z","shell.execute_reply.started":"2026-01-06T15:07:44.399978Z","shell.execute_reply":"2026-01-06T15:07:44.410762Z"}},"outputs":[{"name":"stdout","text":"Class names: ['natural sound', 'unnatural']\nFor values:\n\nnatural sound:0\n\nunnatural:1\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#now we take pretrained model and train them for our data\n\ndata_augmentation = tf.keras.Sequential([\n    # 1. Horizontal Shift (Time Shift) - Very important for forest sounds\n    tf.keras.layers.RandomTranslation(\n        height_factor=0.0, \n        width_factor=0.2, # Be aggressive with time, forest sounds can start anytime\n        fill_mode='wrap'  # 'wrap' is better for audio as it loops the sound\n    ),\n\n    # 2. Small Vertical Shift (Pitch Shift)\n    tf.keras.layers.RandomTranslation(\n        height_factor=0.05, # Keep this small so you don't change species signatures\n        width_factor=0.0,\n        fill_mode='constant'\n    ),\n\n    # 3. Random Noise (Simulates Phone Mic Hiss)\n    tf.keras.layers.GaussianNoise(0.02), \n\n    # 4. Brightness/Contrast (Simulates Different Audio Gains/Volumes)\n    tf.keras.layers.RandomBrightness(factor=0.1),\n    tf.keras.layers.RandomContrast(factor=0.1),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:07:57.815356Z","iopub.execute_input":"2026-01-06T15:07:57.816055Z","iopub.status.idle":"2026-01-06T15:07:57.839416Z","shell.execute_reply.started":"2026-01-06T15:07:57.816027Z","shell.execute_reply":"2026-01-06T15:07:57.838678Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import efficientnet\n\n# 1. Define the Backbone (This is your 'mobile' or 'mobile2')\nbase_model = efficientnet.EfficientNetB0(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights=\"imagenet\"\n)\nbase_model.trainable = False\n\n# 2. Explicitly define the Tensor Flow\ninputs = layers.Input(shape=(128, 1000, 3)) # Match your notebook's spectrograph shape\n\n# Pass through your sequential augmentation block\nx = data_augmentation(inputs) \n\nx = layers.Dropout(0.3)(x)\n\n# Resize and Preprocess\nx = layers.Resizing(224, 224)(x)\nx = layers.Lambda(efficientnet.preprocess_input)(x)\n\n# Pass to EfficientNet - using 'training=False' is safer for frozen layers\nx = base_model(x, training=False)   #<-------------\n\n# Top Layers\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(256, activation=\"relu\")(x)\n# x = layers.Dropout(0.5)(x)\n\n# Choose your output based on which model you are training:\n# For Model 1 (Binary): \noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\n# For Model 2 (3-class: Fire, Logging, Poaching):\n# outputs = layers.Dense(3, activation=\"softmax\")(x)\n\n# 3. Create the Model\nmodel = models.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(optimizer=\"adam\",\n              loss=\"binary_crossentropy\", # Use 'sparse_categorical_crossentropy' for Model 2\n              metrics=[\"accuracy\"])\n\nhistory=model.fit(\n    train_data,\n    validation_data=validation,\n    epochs=50,\n    batch_size=32,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\", #checks for val_accuracy\n            patience=5,#wait tills 5 epochs\n            restore_best_weights=True,#uses best weight\n        ),\n        ModelCheckpoint(\n            \"best_model.keras\",#givesbest model according to val_accuracy\n            monitor=\"val_accuracy\",\n            save_best_only=True,\n            verbose=1 #prints only certain line of epoch for 1 and for 0 is silence and for 2 is every line\n        )\n    ]\n)\ntest_loss,test_acc=model.evaluate(validation)\nprint(f\"Accuracy for validation dataset: {test_acc:4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:08:07.708043Z","iopub.execute_input":"2026-01-06T15:08:07.708796Z","iopub.status.idle":"2026-01-06T15:13:45.004671Z","shell.execute_reply.started":"2026-01-06T15:08:07.708764Z","shell.execute_reply":"2026-01-06T15:13:45.003882Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1767712098.664060      55 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1_1/efficientnetb0_1/block2b_drop_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\nI0000 00:00:1767712101.548996     134 cuda_dnn.cc:529] Loaded cuDNN version 91002\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7243 - loss: 0.5449\nEpoch 1: val_accuracy improved from -inf to 0.75156, saving model to best_model.keras\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 99ms/step - accuracy: 0.7244 - loss: 0.5448 - val_accuracy: 0.7516 - val_loss: 0.5143\nEpoch 2/50\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7772 - loss: 0.4685\nEpoch 2: val_accuracy did not improve from 0.75156\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 88ms/step - accuracy: 0.7773 - loss: 0.4684 - val_accuracy: 0.7063 - val_loss: 0.5185\nEpoch 3/50\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7881 - loss: 0.4411\nEpoch 3: val_accuracy did not improve from 0.75156\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 89ms/step - accuracy: 0.7881 - loss: 0.4411 - val_accuracy: 0.7219 - val_loss: 0.5389\nEpoch 4/50\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7906 - loss: 0.4239\nEpoch 4: val_accuracy did not improve from 0.75156\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.7906 - loss: 0.4239 - val_accuracy: 0.7242 - val_loss: 0.5142\nEpoch 5/50\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7950 - loss: 0.4267\nEpoch 5: val_accuracy did not improve from 0.75156\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.7950 - loss: 0.4266 - val_accuracy: 0.7391 - val_loss: 0.5004\nEpoch 6/50\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7980 - loss: 0.4224\nEpoch 6: val_accuracy improved from 0.75156 to 0.77500, saving model to best_model.keras\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 92ms/step - accuracy: 0.7980 - loss: 0.4224 - val_accuracy: 0.7750 - val_loss: 0.4786\nEpoch 7/50\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8063 - loss: 0.4113\nEpoch 7: val_accuracy did not improve from 0.77500\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.8063 - loss: 0.4113 - val_accuracy: 0.7312 - val_loss: 0.5370\nEpoch 8/50\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8005 - loss: 0.4134\nEpoch 8: val_accuracy did not improve from 0.77500\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.8005 - loss: 0.4134 - val_accuracy: 0.7437 - val_loss: 0.5472\nEpoch 9/50\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8032 - loss: 0.4023\nEpoch 9: val_accuracy did not improve from 0.77500\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.8032 - loss: 0.4022 - val_accuracy: 0.7633 - val_loss: 0.4975\nEpoch 10/50\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8153 - loss: 0.3929\nEpoch 10: val_accuracy did not improve from 0.77500\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.8153 - loss: 0.3930 - val_accuracy: 0.7164 - val_loss: 0.5428\nEpoch 11/50\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8115 - loss: 0.3888\nEpoch 11: val_accuracy did not improve from 0.77500\n\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.8115 - loss: 0.3888 - val_accuracy: 0.7352 - val_loss: 0.5467\n\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.7758 - loss: 0.4829\nAccuracy for validation dataset: 0.780469\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"result=model.evaluate(test)\nprint(f\"For the unseen test data of the entire training this model has accuracy of {result[1]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:14:49.687429Z","iopub.execute_input":"2026-01-06T15:14:49.688072Z","iopub.status.idle":"2026-01-06T15:14:54.535616Z","shell.execute_reply.started":"2026-01-06T15:14:49.688044Z","shell.execute_reply":"2026-01-06T15:14:54.535033Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - accuracy: 0.7728 - loss: 0.4869\nFor the unseen test data of the entire training this model has accuracy of 0.7721\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/models\",exist_ok=True)\nmodel.save(\"/kaggle/working/models/audio_forest.keras\")\nimport shutil\n\n# Zip the folder for easy download\nshutil.make_archive(\"/kaggle/working/models\", 'zip', \"/kaggle/working/models\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:15:28.276528Z","iopub.execute_input":"2026-01-06T15:15:28.277243Z","iopub.status.idle":"2026-01-06T15:15:29.823592Z","shell.execute_reply.started":"2026-01-06T15:15:28.277188Z","shell.execute_reply":"2026-01-06T15:15:29.822945Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/models.zip'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"selected_class=[\"fire\",\"logging\", \"poaching\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:16:31.761278Z","iopub.execute_input":"2026-01-06T15:16:31.761858Z","iopub.status.idle":"2026-01-06T15:16:31.765294Z","shell.execute_reply.started":"2026-01-06T15:16:31.761827Z","shell.execute_reply":"2026-01-06T15:16:31.764622Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#loading up images and dividing them for the neural net\ntrain_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    image_size=(128, 1000),\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=SEED,\n    batch_size=BATCH_SIZE,\n    label_mode=\"int\" ,\n    color_mode='rgb'\n)\n\n#validation dataset\nvalidation_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    image_size=(128, 1000),\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=SEED,\n    batch_size=BATCH_SIZE,\n    label_mode=\"int\",\n    color_mode='rgb'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:16:35.706412Z","iopub.execute_input":"2026-01-06T15:16:35.707127Z","iopub.status.idle":"2026-01-06T15:16:41.675719Z","shell.execute_reply.started":"2026-01-06T15:16:35.707100Z","shell.execute_reply":"2026-01-06T15:16:41.674981Z"}},"outputs":[{"name":"stdout","text":"Found 5768 files belonging to 3 classes.\nUsing 4615 files for training.\nFound 5768 files belonging to 3 classes.\nUsing 1153 files for validation.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"#now dividing the validation data set into \ntotal_number_of_batches_in_validation_data=validation_data.cardinality().numpy() #converts total number of batches in the set and converts the tensor into python integer with .numpy function here\nno_of_batches_in_validation_data=total_number_of_batches_in_validation_data//2 #this is a floor division operator\n\n\n#now from the set of batches of the image creating subset into the validation and the test subset\nvalidation=validation_data.take(no_of_batches_in_validation_data)\ntest=validation_data.skip(no_of_batches_in_validation_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:16:41.676845Z","iopub.execute_input":"2026-01-06T15:16:41.677034Z","iopub.status.idle":"2026-01-06T15:16:41.684631Z","shell.execute_reply.started":"2026-01-06T15:16:41.677016Z","shell.execute_reply":"2026-01-06T15:16:41.683977Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(f\"Class names: {train_data.class_names}\")\nprint(\"For values:\\n\")\nfor i, class_name in enumerate(train_data.class_names):\n    print(f\"{class_name}:{i}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:16:50.654093Z","iopub.execute_input":"2026-01-06T15:16:50.654683Z","iopub.status.idle":"2026-01-06T15:16:50.659240Z","shell.execute_reply.started":"2026-01-06T15:16:50.654654Z","shell.execute_reply":"2026-01-06T15:16:50.658507Z"}},"outputs":[{"name":"stdout","text":"Class names: ['fire', 'logging', 'poaching']\nFor values:\n\nfire:0\n\nlogging:1\n\npoaching:2\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"\ndef multi_class_model():\n    # 1. Define Input\n    inputs = tf.keras.Input(shape=(128, 1000, 3))\n    x = data_augmentation(inputs)\n    x = layers.Resizing(224, 224)(x)\n\n\n    base_model2 = tf.keras.applications.EfficientNetB0(\n        input_shape=(224, 224, 3),\n        include_top=False,\n        weights=\"imagenet\"\n    )\n    base_model2.trainable = False\n    \n    x = base_model2(x, training=False) # training=False keeps BatchNormalization in inference mode\n\n    # 5. The \"Head\"\n    x = layers.GlobalMaxPooling2D()(x)\n    x = layers.Dense(256, activation=\"relu\")(x)\n    outputs = layers.Dense(3, activation=\"softmax\")(x)\n\n    # 6. Create the Model\n    model2 = models.Model(inputs=inputs, outputs=outputs)\n    \n    return model2\nmodel2 = multi_class_model()\n\nmodel2.compile(optimizer=\"adam\",\n             loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\n\nhistory=model2.fit(\n    train_data,\n    validation_data=validation,\n    epochs=50,\n    batch_size=32,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\", #checks for val_accuracy\n            patience=5,#wait tills 5 epochs\n            restore_best_weights=True,#uses best weight\n        ),\n        ModelCheckpoint(\n            \"best_model.keras\",#givesbest model according to val_accuracy\n            monitor=\"val_accuracy\",\n            save_best_only=True,\n            verbose=1 #prints only certain line of epoch for 1 and for 0 is silence and for 2 is every line\n        )\n    ]\n)\n\ntest_loss,test_acc=model2.evaluate(validation)\nprint(f\"Accuracy for validation dataset: {test_acc:4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:17:02.114460Z","iopub.execute_input":"2026-01-06T15:17:02.115033Z","iopub.status.idle":"2026-01-06T15:22:21.451693Z","shell.execute_reply.started":"2026-01-06T15:17:02.115005Z","shell.execute_reply":"2026-01-06T15:22:21.451043Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1767712631.787478      55 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_2_1/efficientnetb0_1/block2b_drop_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7486 - loss: 1.1996\nEpoch 1: val_accuracy improved from -inf to 0.90451, saving model to best_model.keras\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 113ms/step - accuracy: 0.7494 - loss: 1.1921 - val_accuracy: 0.9045 - val_loss: 0.2311\nEpoch 2/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8489 - loss: 0.3604\nEpoch 2: val_accuracy improved from 0.90451 to 0.90972, saving model to best_model.keras\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 94ms/step - accuracy: 0.8490 - loss: 0.3601 - val_accuracy: 0.9097 - val_loss: 0.1991\nEpoch 3/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8839 - loss: 0.2882\nEpoch 3: val_accuracy did not improve from 0.90972\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 90ms/step - accuracy: 0.8840 - loss: 0.2881 - val_accuracy: 0.8958 - val_loss: 0.2696\nEpoch 4/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8897 - loss: 0.2714\nEpoch 4: val_accuracy improved from 0.90972 to 0.91493, saving model to best_model.keras\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 95ms/step - accuracy: 0.8898 - loss: 0.2712 - val_accuracy: 0.9149 - val_loss: 0.1797\nEpoch 5/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8822 - loss: 0.2760\nEpoch 5: val_accuracy improved from 0.91493 to 0.92882, saving model to best_model.keras\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - accuracy: 0.8823 - loss: 0.2757 - val_accuracy: 0.9288 - val_loss: 0.1705\nEpoch 6/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8958 - loss: 0.2550\nEpoch 6: val_accuracy did not improve from 0.92882\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.8959 - loss: 0.2549 - val_accuracy: 0.9184 - val_loss: 0.2092\nEpoch 7/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8960 - loss: 0.2622\nEpoch 7: val_accuracy improved from 0.92882 to 0.93403, saving model to best_model.keras\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 94ms/step - accuracy: 0.8961 - loss: 0.2620 - val_accuracy: 0.9340 - val_loss: 0.1824\nEpoch 8/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9092 - loss: 0.2174\nEpoch 8: val_accuracy improved from 0.93403 to 0.94792, saving model to best_model.keras\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 94ms/step - accuracy: 0.9092 - loss: 0.2175 - val_accuracy: 0.9479 - val_loss: 0.1429\nEpoch 9/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9092 - loss: 0.2199\nEpoch 9: val_accuracy did not improve from 0.94792\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9092 - loss: 0.2199 - val_accuracy: 0.9444 - val_loss: 0.1504\nEpoch 10/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9078 - loss: 0.2218\nEpoch 10: val_accuracy did not improve from 0.94792\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9078 - loss: 0.2219 - val_accuracy: 0.9219 - val_loss: 0.1851\nEpoch 11/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9126 - loss: 0.2225\nEpoch 11: val_accuracy did not improve from 0.94792\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9126 - loss: 0.2224 - val_accuracy: 0.9340 - val_loss: 0.1635\nEpoch 12/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9221 - loss: 0.2053\nEpoch 12: val_accuracy did not improve from 0.94792\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9221 - loss: 0.2053 - val_accuracy: 0.9340 - val_loss: 0.1827\nEpoch 13/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9119 - loss: 0.2054\nEpoch 13: val_accuracy improved from 0.94792 to 0.94965, saving model to best_model.keras\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 94ms/step - accuracy: 0.9119 - loss: 0.2055 - val_accuracy: 0.9497 - val_loss: 0.1259\nEpoch 14/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9125 - loss: 0.2098\nEpoch 14: val_accuracy did not improve from 0.94965\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9126 - loss: 0.2098 - val_accuracy: 0.9375 - val_loss: 0.1595\nEpoch 15/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9205 - loss: 0.1904\nEpoch 15: val_accuracy did not improve from 0.94965\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9205 - loss: 0.1905 - val_accuracy: 0.9392 - val_loss: 0.1397\nEpoch 16/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9179 - loss: 0.1973\nEpoch 16: val_accuracy did not improve from 0.94965\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9179 - loss: 0.1974 - val_accuracy: 0.9167 - val_loss: 0.2259\nEpoch 17/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9162 - loss: 0.2002\nEpoch 17: val_accuracy did not improve from 0.94965\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9162 - loss: 0.2003 - val_accuracy: 0.8993 - val_loss: 0.2288\nEpoch 18/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9171 - loss: 0.2021\nEpoch 18: val_accuracy improved from 0.94965 to 0.95312, saving model to best_model.keras\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 94ms/step - accuracy: 0.9171 - loss: 0.2020 - val_accuracy: 0.9531 - val_loss: 0.1243\nEpoch 19/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9203 - loss: 0.2140\nEpoch 19: val_accuracy did not improve from 0.95312\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9203 - loss: 0.2139 - val_accuracy: 0.9358 - val_loss: 0.1832\nEpoch 20/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9125 - loss: 0.2080\nEpoch 20: val_accuracy did not improve from 0.95312\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9126 - loss: 0.2079 - val_accuracy: 0.9427 - val_loss: 0.1476\nEpoch 21/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9248 - loss: 0.1909\nEpoch 21: val_accuracy did not improve from 0.95312\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9248 - loss: 0.1908 - val_accuracy: 0.9236 - val_loss: 0.1948\nEpoch 22/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9220 - loss: 0.1938\nEpoch 22: val_accuracy did not improve from 0.95312\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9221 - loss: 0.1936 - val_accuracy: 0.9462 - val_loss: 0.1345\nEpoch 23/50\n\u001b[1m144/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9288 - loss: 0.1765\nEpoch 23: val_accuracy did not improve from 0.95312\n\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9287 - loss: 0.1767 - val_accuracy: 0.9306 - val_loss: 0.1827\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9511 - loss: 0.1318\nAccuracy for validation dataset: 0.946181\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"result=model2.evaluate(test)\nprint(f\"For the unseen test data of the entire training this model has accuracy of {result[1]:.4f}\\\\\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:22:39.197164Z","iopub.execute_input":"2026-01-06T15:22:39.197862Z","iopub.status.idle":"2026-01-06T15:22:41.723951Z","shell.execute_reply.started":"2026-01-06T15:22:39.197832Z","shell.execute_reply":"2026-01-06T15:22:41.723379Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9443 - loss: 0.1713\nFor the unseen test data of the entire training this model has accuracy of 0.9411\\\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"result=model2.evaluate(train_data)\nprint(f\"For the unseen test data of the entire training this model has accuracy of {result[1]:.4f}\\\\\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T15:23:25.461666Z","iopub.execute_input":"2026-01-06T15:23:25.462192Z","iopub.status.idle":"2026-01-06T15:23:35.713572Z","shell.execute_reply.started":"2026-01-06T15:23:25.462165Z","shell.execute_reply":"2026-01-06T15:23:35.713014Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step - accuracy: 0.9321 - loss: 0.1869\nFor the unseen test data of the entire training this model has accuracy of 0.9333\\\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"\n# ===== ADD THIS CELL AFTER model.fit() COMPLETES =====\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ðŸ” VERIFYING MODEL BEFORE SAVING\")\nprint(\"=\"*80)\n\n# Check the model we just trained\nprint(f\"Model input shape: {model2.input_shape}\")\nprint(f\"Expected:          (None, 128, 1000, 3)\")\n\n# Test with a sample batch\nfor images, labels in train_data.take(1):\n    print(f\"\\nSample batch shape: {images.shape}\")\n    print(f\"Sample batch channels: {images.shape[-1]}\")\n    \n    # Try prediction\n    try:\n        pred = model.predict(images[:1])\n        print(f\"âœ… Model can predict on training data\")\n        print(f\"   Prediction shape: {pred.shape}\")\n    except Exception as e:\n        print(f\"âŒ Model CANNOT predict: {e}\")\n        print(\"   TRAINING DATA AND MODEL DON'T MATCH!\")\n\nif model2.input_shape == (None, 128, 1000, 3):\n    print(\"\\nâœ… MODEL ARCHITECTURE IS CORRECT\")\nelse:\n    print(f\"\\nâŒ MODEL ARCHITECTURE IS WRONG!\")\n    print(f\"   Model expects: {model2.input_shape}\")\n    print(f\"   But should be: (None, 128, 1000, 3)\")\n    raise ValueError(\"Model has wrong input shape! Fix training code!\")\n\nprint(\"=\"*80 + \"\\n\")\n\n# Now save\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\nmodel2.save(\"/kaggle/working/models/audio_forest_69.keras\")\n\n# Verify the SAVED file\nprint(\"\\n\" + \"=\"*80)\nprint(\"ðŸ” VERIFYING SAVED FILE\")\nprint(\"=\"*80)\n\ntest_model = tf.keras.models.load_model(\n    \"/kaggle/working/models/audio_forest_69.keras\", \n    compile=False\n)\n\nprint(f\"Saved model input shape: {test_model.input_shape}\")\n\nif test_model.input_shape == (None, 128, 1000, 3):\n    print(\"âœ… SAVED MODEL IS CORRECT - Safe to download!\")\nelse:\n    print(\"âŒ SAVED MODEL IS WRONG!\")\n    raise ValueError(\"Something went wrong during save!\")\n    \nprint(\"=\"*80 + \"\\n\")\n\n# Zip it\nimport shutil\nshutil.make_archive(\"/kaggle/working/models\", 'zip', \"/kaggle/working/models\")\nprint(\"âœ… models.zip created - MODEL IS VERIFIED CORRECT!\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/models\",exist_ok=True)\nmodel.save(\"/kaggle/working/models/audio_multi_classification.keras\")\nimport shutil\n\n# Zip the folder for easy download\nshutil.make_archive(\"/kaggle/working/maheshdalle\", 'zip', \"/kaggle/working/models\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}